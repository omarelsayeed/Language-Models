{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('nlp-getting-started/train_Arabic_tweets_positive_20190413.tsv ' , delimiter='\\t',header =None , names = ['x' , 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes Punctuations\n",
    "def remove_punctuations(data):\n",
    "    punct_tag=re.compile(r'[^\\w\\s]')\n",
    "    data = punct_tag.sub(r'',data)\n",
    "    return data\n",
    "    \n",
    "#Removes HTML syntaxes\n",
    "def remove_html(data):\n",
    "    html_tag=re.compile(r'<.*?>')\n",
    "    data=html_tag.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "#Removes URL data\n",
    "def remove_url(data):\n",
    "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    data=url_clean.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "#Removes Emojis\n",
    "def remove_emoji(data):\n",
    "    emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    data=emoji_clean.sub(r'',data)\n",
    "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    data=url_clean.sub(r'',data) \n",
    "    return data\n",
    "\n",
    "dataset['text']=dataset['text'].apply(lambda z: remove_html(z))\n",
    "dataset['text']=dataset['text'].apply(lambda z: remove_punctuations(z))\n",
    "dataset['text']=dataset['text'].apply(lambda z: remove_url(z))\n",
    "dataset['text']=dataset['text'].apply(lambda z: remove_emoji(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punc\n",
    "dataset['text'] = dataset.text.apply(lambda x : ''.join([char for char in x if char not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace _ by whitespaces \n",
    "dataset['text'] = dataset.text.apply(lambda x : x.replace('_' , ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace multiple whitespaces by one\n",
    "dataset['text'] = dataset.text.apply(lambda x : ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_abb(data):\n",
    "    abb_clean= re.compile(r\"\\b[a-zA-Z]\\.[a-zA-Z]\\b\")\n",
    "    data=abb_clean.sub(r'',data)\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda z: remove_abb(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset.text.apply(lambda z : z.replace('pos' , ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>وفي النهاية لن يبقى معك آحدإلا من رأى الجمال ف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>من الخير نفسه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>زلزلالملعبنصرنابيلعب كن عالي الهمه ولا ترضى بغ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو المسيا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22756</th>\n",
       "      <td>pos</td>\n",
       "      <td>السحب الليلة على الايفون رتويت للمرفقة وطبق ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22757</th>\n",
       "      <td>pos</td>\n",
       "      <td>لابسة احمر ليه يا ست انتي ايه المناسبة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22758</th>\n",
       "      <td>pos</td>\n",
       "      <td>كلاام جمييل تستاهلمن احبه الله جعل محبته ف قلو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>pos</td>\n",
       "      <td>ألطف صورة ممكن تعبر عن رمضان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22760</th>\n",
       "      <td>pos</td>\n",
       "      <td>قال الإمامابنالقيم رحمه الله تعالى فإن من لم ي...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22761 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x                                               text\n",
       "0      pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n",
       "1      pos  وفي النهاية لن يبقى معك آحدإلا من رأى الجمال ف...\n",
       "2      pos                                      من الخير نفسه\n",
       "3      pos  زلزلالملعبنصرنابيلعب كن عالي الهمه ولا ترضى بغ...\n",
       "4      pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو المسيا...\n",
       "...    ...                                                ...\n",
       "22756  pos  السحب الليلة على الايفون رتويت للمرفقة وطبق ال...\n",
       "22757  pos             لابسة احمر ليه يا ست انتي ايه المناسبة\n",
       "22758  pos  كلاام جمييل تستاهلمن احبه الله جعل محبته ف قلو...\n",
       "22759  pos                       ألطف صورة ممكن تعبر عن رمضان\n",
       "22760  pos  قال الإمامابنالقيم رحمه الله تعالى فإن من لم ي...\n",
       "\n",
       "[22761 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramsLM(object):\n",
    "\n",
    "    def __init__(self , data : pd.Series , n_grams : int):\n",
    "        self.data = data\n",
    "        self.n = n_grams\n",
    "        self.grams_dictionary = self.get_n_grams(self.n)\n",
    "\n",
    "    def get_n_grams(self , n):\n",
    "\n",
    "        from collections import defaultdict\n",
    "        d = defaultdict(int)\n",
    "        for gram_length in range(1 , n+1):\n",
    "            for item in self.data:\n",
    "                item= item.split()\n",
    "                for i in range(len(item)):\n",
    "                    d[\" \".join(item[i:i+gram_length])] +=1\n",
    "        return dict((x,y) for (x,y) in sorted(d.items() , key = lambda z : z[1] , reverse= True))\n",
    "\n",
    "    def get_recommendations(self , text :str , n  : int = 5 ):\n",
    "        L = [item for item in self.grams_dictionary.keys() if item.startswith(text) and item != text][:n]\n",
    "        return dict(zip( L ,range(1 , len(L) +1 ) ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngLM = NGramsLM(dataset.text ,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'صباح الخير': 1,\n",
       " 'صباح النور': 2,\n",
       " 'صباح الورد': 3,\n",
       " 'صباح الخير حسينعبدالغني': 4,\n",
       " 'صباح الخير ساعة': 5,\n",
       " 'صباح الخير ساعة تماما': 6,\n",
       " 'صباح الفل': 7,\n",
       " 'صباح الخير لمن': 8,\n",
       " 'صباح السعادة': 9,\n",
       " 'صباح النور والسرور': 10,\n",
       " 'صباح خير': 11,\n",
       " 'صباح الابتسامة': 12,\n",
       " 'صباح الخيير': 13,\n",
       " 'صباح الخيرات': 14,\n",
       " 'صباح السعاده': 15,\n",
       " 'صباح الرضا': 16,\n",
       " 'صباح جميل': 17,\n",
       " 'صباح المطر': 18,\n",
       " 'صباح الخيرر': 19,\n",
       " 'صباح النصر': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text =\"صباح \"  \n",
    "ngLM.get_recommendations(my_text , 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2b1884aac98f9b7855e8895f74d488ad518f49fce9c97dd1a17c527c00def65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
