{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NHvCAa_tbEmx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "siGtgplfXKR_"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('train_Arabic_tweets_positive_20190413.tsv' , delimiter='\\t',header =None , names = ['x' , 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wdyQXUZLdMpj"
      },
      "outputs": [],
      "source": [
        "#Removes HTML syntaxes\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes URL data\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "#Removes Emojis\n",
        "def remove_emoji(data):\n",
        "    emoji_clean= re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    data=emoji_clean.sub(r'',data)\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "dataset['text']=dataset['text'].apply(lambda z: remove_html(z))\n",
        "dataset['text']=dataset['text'].apply(lambda z: remove_url(z))\n",
        "dataset['text']=dataset['text'].apply(lambda z: remove_emoji(z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jFLEmco4dPBS"
      },
      "outputs": [],
      "source": [
        "# replace _ by whitespaces \n",
        "dataset['text'] = dataset.text.apply(lambda x : x.replace('_' , ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1xktxBx7dQFC"
      },
      "outputs": [],
      "source": [
        "# replace multiple whitespaces by one\n",
        "dataset['text'] = dataset.text.apply(lambda x : ' '.join(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mqpLev9RdRIq"
      },
      "outputs": [],
      "source": [
        "def remove_abb(data):\n",
        "    abb_clean= re.compile(r\"\\b[a-zA-Z]\\.[a-zA-Z]\\b\")\n",
        "    data=abb_clean.sub(r'',data)\n",
        "    return data\n",
        "dataset['text'] = dataset['text'].apply(lambda z: remove_abb(z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1vQNnj50dSE6"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.text.apply(lambda z : z.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hDpe3NxQdTxi"
      },
      "outputs": [],
      "source": [
        "#remove any word that starts with http\n",
        "dataset=dataset.apply(lambda z: ' '.join([word for word in z.split() if not word.startswith('http')]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTpQiEOndbJq",
        "outputId": "c70cbcfd-9f8e-4cb2-afb7-86a757cac030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n",
              "1        وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n",
              "2                                            من الخير نفسه\n",
              "3        #زلزل الملعب نصرنا بيلعب كن عالي الهمه ولا ترض...\n",
              "4        الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n",
              "                               ...                        \n",
              "22756    السحب الليلة على الايفون .. رتويت للمرفقة وطبق...\n",
              "22757               لابسة احمر ليه يا ست انتي ايه المناسبة\n",
              "22758    كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...\n",
              "22759                       - ألطف صورة ممكن تعبر عن رمضان\n",
              "22760    قال #الإمام ابن القيم -رحمه الله تعالى- : - \" ...\n",
              "Name: text, Length: 22761, dtype: object"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vai60l-2g9Sq"
      },
      "outputs": [],
      "source": [
        "tokens = dataset[0].lower().split()\n",
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "for item in dataset:\n",
        "    tokenz = item.lower().split()\n",
        "    for token in tokenz:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "dataset['text_tokenized'] = dataset.apply(lambda x: [vocab[x] for x in x.lower().split()])\n",
        "vocab_size = max(vocab.values())+ 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rKLm68ktz3l",
        "outputId": "279777c3-7964-4b76-8ab6-370e74c9dd95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48631"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I1YiIwvMhNO7"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "for j in range(len(dataset)):\n",
        "    try :\n",
        "        for i in range(len(dataset.text_tokenized[j])):\n",
        "            item = dataset.text_tokenized[j]\n",
        "            if len(item[i:i+3]) == 3 and item[i+3] != None :\n",
        "                X.append(item[i:i+3]) \n",
        "                Y.append(item[i+3])\n",
        "    except : pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoTjzX3OiuID",
        "outputId": "a5e353f4-004d-4116-89e0-264cacb8f9c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "234140"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rXstmnf3jEJj"
      },
      "outputs": [],
      "source": [
        "class KerasBatchGenerator(object):\n",
        "    def __init__(self , X,Y , batch_size , vocab_size , num_steps = 3):\n",
        "      self.X = X\n",
        "      self.Y = Y\n",
        "      self.batch_size = batch_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.current_idx = 0\n",
        "      self.num_steps = num_steps\n",
        "\n",
        "    def generate(self):\n",
        "\n",
        "        while True:\n",
        "                if self.current_idx + 128 >= len(self.X):\n",
        "                    # reset the index back to the start of the data set\n",
        "                    self.current_idx = 0\n",
        "                x = self.X[self.current_idx : self.current_idx + 128]\n",
        "                x = np.array(x)\n",
        "                # convert all of temp_y into a one hot representation\n",
        "                y = to_categorical(Y[self.current_idx : self.current_idx + 128], num_classes=self.vocab_size)\n",
        "                self.current_idx += 128\n",
        "                yield (x, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_TOAqbl5hb0j"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding , LSTM , Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "model = Sequential(\n",
        "    [\n",
        "        Embedding(vocab_size , 128 , input_length= 3) ,\n",
        "        LSTM(128 , return_sequences = True ),\n",
        "        LSTM(64  , return_sequences = True),\n",
        "        LSTM(32 , return_sequences = True),\n",
        "        LSTM(16 ),\n",
        "        Dense(vocab_size, activation = 'softmax')\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "40U31UAUwpVh"
      },
      "outputs": [],
      "source": [
        "gen = KerasBatchGenerator(X , Y , 128 , vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6YxvsXjhu9q",
        "outputId": "0daf1d64-d2b8-4dc2-f56c-c9cc2fb5b334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1829/1829 [==============================] - 298s 157ms/step - loss: 9.0209 - accuracy: 0.0182\n",
            "Epoch 2/100\n",
            "1829/1829 [==============================] - 299s 163ms/step - loss: 8.3485 - accuracy: 0.0209\n",
            "Epoch 3/100\n",
            "1829/1829 [==============================] - 304s 166ms/step - loss: 7.9751 - accuracy: 0.0249\n",
            "Epoch 4/100\n",
            "1829/1829 [==============================] - 317s 173ms/step - loss: 7.6736 - accuracy: 0.0301\n",
            "Epoch 5/100\n",
            "1829/1829 [==============================] - 322s 176ms/step - loss: 7.4322 - accuracy: 0.0331\n",
            "Epoch 6/100\n",
            "1829/1829 [==============================] - 321s 176ms/step - loss: 7.2250 - accuracy: 0.0360\n",
            "Epoch 7/100\n",
            "1829/1829 [==============================] - 312s 170ms/step - loss: 7.0326 - accuracy: 0.0419\n",
            "Epoch 8/100\n",
            "1829/1829 [==============================] - 307s 168ms/step - loss: 6.8534 - accuracy: 0.0494\n",
            "Epoch 9/100\n",
            "1829/1829 [==============================] - 314s 172ms/step - loss: 6.6832 - accuracy: 0.0552\n",
            "Epoch 10/100\n",
            "1829/1829 [==============================] - 293s 160ms/step - loss: 6.5254 - accuracy: 0.0601\n",
            "Epoch 11/100\n",
            "1829/1829 [==============================] - 292s 160ms/step - loss: 6.3745 - accuracy: 0.0652\n",
            "Epoch 12/100\n",
            "1829/1829 [==============================] - 290s 159ms/step - loss: 6.2392 - accuracy: 0.0712\n",
            "Epoch 13/100\n",
            "1829/1829 [==============================] - 292s 159ms/step - loss: 6.1120 - accuracy: 0.0765\n",
            "Epoch 14/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.9916 - accuracy: 0.0826\n",
            "Epoch 15/100\n",
            "1829/1829 [==============================] - 290s 159ms/step - loss: 5.8742 - accuracy: 0.0887\n",
            "Epoch 16/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.7562 - accuracy: 0.0948\n",
            "Epoch 17/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.6440 - accuracy: 0.1012\n",
            "Epoch 18/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.5375 - accuracy: 0.1095\n",
            "Epoch 19/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.4327 - accuracy: 0.1170\n",
            "Epoch 20/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.3279 - accuracy: 0.1248\n",
            "Epoch 21/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.2320 - accuracy: 0.1316\n",
            "Epoch 22/100\n",
            "1829/1829 [==============================] - 291s 159ms/step - loss: 5.1370 - accuracy: 0.1376\n",
            "Epoch 23/100\n",
            "1829/1829 [==============================] - 294s 161ms/step - loss: 5.0432 - accuracy: 0.1467\n",
            "Epoch 24/100\n",
            "1829/1829 [==============================] - 293s 160ms/step - loss: 4.9504 - accuracy: 0.1591\n",
            "Epoch 25/100\n",
            "1829/1829 [==============================] - 292s 160ms/step - loss: 4.8555 - accuracy: 0.1690\n",
            "Epoch 26/100\n",
            "1829/1829 [==============================] - 293s 160ms/step - loss: 4.7707 - accuracy: 0.1793\n",
            "Epoch 27/100\n",
            "1829/1829 [==============================] - 294s 161ms/step - loss: 4.6865 - accuracy: 0.1885\n",
            "Epoch 28/100\n",
            "1829/1829 [==============================] - 295s 161ms/step - loss: 4.6066 - accuracy: 0.1985\n",
            "Epoch 29/100\n",
            "1829/1829 [==============================] - 295s 161ms/step - loss: 4.5292 - accuracy: 0.2081\n",
            "Epoch 30/100\n",
            "1829/1829 [==============================] - 295s 161ms/step - loss: 4.4566 - accuracy: 0.2178\n",
            "Epoch 31/100\n",
            "1829/1829 [==============================] - 295s 161ms/step - loss: 4.3837 - accuracy: 0.2276\n",
            "Epoch 32/100\n",
            "1829/1829 [==============================] - 294s 161ms/step - loss: 4.3137 - accuracy: 0.2382\n",
            "Epoch 33/100\n",
            "1829/1829 [==============================] - 294s 161ms/step - loss: 4.2449 - accuracy: 0.2496\n",
            "Epoch 34/100\n",
            "1829/1829 [==============================] - 295s 161ms/step - loss: 4.1765 - accuracy: 0.2597\n",
            "Epoch 35/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 4.1081 - accuracy: 0.2701\n",
            "Epoch 36/100\n",
            "1829/1829 [==============================] - 296s 162ms/step - loss: 4.0451 - accuracy: 0.2788\n",
            "Epoch 37/100\n",
            "1829/1829 [==============================] - 296s 162ms/step - loss: 3.9830 - accuracy: 0.2874\n",
            "Epoch 38/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.9214 - accuracy: 0.2974\n",
            "Epoch 39/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.8594 - accuracy: 0.3069\n",
            "Epoch 40/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.7978 - accuracy: 0.3159\n",
            "Epoch 41/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.7309 - accuracy: 0.3289\n",
            "Epoch 42/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.6717 - accuracy: 0.3378\n",
            "Epoch 43/100\n",
            "1829/1829 [==============================] - 297s 162ms/step - loss: 3.6126 - accuracy: 0.3477\n",
            "Epoch 44/100\n",
            "1829/1829 [==============================] - 298s 163ms/step - loss: 3.5582 - accuracy: 0.3567\n",
            "Epoch 45/100\n",
            "1829/1829 [==============================] - 298s 163ms/step - loss: 3.5074 - accuracy: 0.3654\n",
            "Epoch 46/100\n",
            "1829/1829 [==============================] - 298s 163ms/step - loss: 3.4561 - accuracy: 0.3738\n",
            "Epoch 47/100\n",
            "1829/1829 [==============================] - 298s 163ms/step - loss: 3.4045 - accuracy: 0.3821\n",
            "Epoch 48/100\n",
            "1829/1829 [==============================] - 301s 165ms/step - loss: 3.3573 - accuracy: 0.3895\n",
            "Epoch 49/100\n",
            "1829/1829 [==============================] - 299s 163ms/step - loss: 3.3086 - accuracy: 0.3976\n",
            "Epoch 50/100\n",
            "1829/1829 [==============================] - 299s 164ms/step - loss: 3.2636 - accuracy: 0.4051\n",
            "Epoch 51/100\n",
            "1829/1829 [==============================] - 300s 164ms/step - loss: 3.2181 - accuracy: 0.4114\n",
            "Epoch 52/100\n",
            "1829/1829 [==============================] - 300s 164ms/step - loss: 3.1749 - accuracy: 0.4191\n",
            "Epoch 53/100\n",
            "1829/1829 [==============================] - 300s 164ms/step - loss: 3.1353 - accuracy: 0.4254\n",
            "Epoch 54/100\n",
            "1829/1829 [==============================] - 301s 164ms/step - loss: 3.0938 - accuracy: 0.4319\n",
            "Epoch 55/100\n",
            "1829/1829 [==============================] - 300s 164ms/step - loss: 3.0552 - accuracy: 0.4384\n",
            "Epoch 56/100\n",
            "1829/1829 [==============================] - 301s 164ms/step - loss: 3.0137 - accuracy: 0.4452\n",
            "Epoch 57/100\n",
            "1829/1829 [==============================] - 301s 165ms/step - loss: 2.9799 - accuracy: 0.4506\n",
            "Epoch 58/100\n",
            "1829/1829 [==============================] - 303s 165ms/step - loss: 2.9446 - accuracy: 0.4565\n",
            "Epoch 59/100\n",
            "1829/1829 [==============================] - 301s 165ms/step - loss: 2.9062 - accuracy: 0.4624\n",
            "Epoch 60/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.8729 - accuracy: 0.4692\n",
            "Epoch 61/100\n",
            "1829/1829 [==============================] - 301s 165ms/step - loss: 2.8400 - accuracy: 0.4743\n",
            "Epoch 62/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.8108 - accuracy: 0.4794\n",
            "Epoch 63/100\n",
            "1829/1829 [==============================] - 301s 165ms/step - loss: 2.7770 - accuracy: 0.4842\n",
            "Epoch 64/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.7451 - accuracy: 0.4904\n",
            "Epoch 65/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.7169 - accuracy: 0.4950\n",
            "Epoch 66/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.6869 - accuracy: 0.4994\n",
            "Epoch 67/100\n",
            "1829/1829 [==============================] - 303s 165ms/step - loss: 2.6588 - accuracy: 0.5045\n",
            "Epoch 68/100\n",
            "1829/1829 [==============================] - 304s 166ms/step - loss: 2.6298 - accuracy: 0.5098\n",
            "Epoch 69/100\n",
            "1829/1829 [==============================] - 303s 165ms/step - loss: 2.6027 - accuracy: 0.5145\n",
            "Epoch 70/100\n",
            "1829/1829 [==============================] - 302s 165ms/step - loss: 2.5760 - accuracy: 0.5195\n",
            "Epoch 71/100\n",
            "1829/1829 [==============================] - 305s 167ms/step - loss: 2.5494 - accuracy: 0.5243\n",
            "Epoch 72/100\n",
            "1829/1829 [==============================] - 303s 166ms/step - loss: 2.5226 - accuracy: 0.5293\n",
            "Epoch 73/100\n",
            "1829/1829 [==============================] - 303s 165ms/step - loss: 2.4967 - accuracy: 0.5335\n",
            "Epoch 74/100\n",
            "1829/1829 [==============================] - 304s 166ms/step - loss: 2.4715 - accuracy: 0.5382\n",
            "Epoch 75/100\n",
            "1829/1829 [==============================] - 304s 166ms/step - loss: 2.4462 - accuracy: 0.5421\n",
            "Epoch 76/100\n",
            "1829/1829 [==============================] - 305s 167ms/step - loss: 2.4244 - accuracy: 0.5464\n",
            "Epoch 77/100\n",
            "1829/1829 [==============================] - 305s 167ms/step - loss: 2.4009 - accuracy: 0.5517\n",
            "Epoch 78/100\n",
            "1829/1829 [==============================] - 305s 167ms/step - loss: 2.3768 - accuracy: 0.5554\n",
            "Epoch 79/100\n",
            "1829/1829 [==============================] - 305s 167ms/step - loss: 2.3557 - accuracy: 0.5587\n",
            "Epoch 80/100\n",
            "1829/1829 [==============================] - 306s 167ms/step - loss: 2.3333 - accuracy: 0.5628\n",
            "Epoch 81/100\n",
            "1829/1829 [==============================] - 306s 167ms/step - loss: 2.3093 - accuracy: 0.5675\n",
            "Epoch 82/100\n",
            "1829/1829 [==============================] - 308s 169ms/step - loss: 2.2904 - accuracy: 0.5698\n",
            "Epoch 83/100\n",
            "1829/1829 [==============================] - 310s 170ms/step - loss: 2.2704 - accuracy: 0.5741\n",
            "Epoch 84/100\n",
            "1829/1829 [==============================] - 307s 168ms/step - loss: 2.2482 - accuracy: 0.5781\n",
            "Epoch 85/100\n",
            "1829/1829 [==============================] - 307s 168ms/step - loss: 2.2284 - accuracy: 0.5810\n",
            "Epoch 86/100\n",
            "1829/1829 [==============================] - 307s 168ms/step - loss: 2.2095 - accuracy: 0.5845\n",
            "Epoch 87/100\n",
            "1829/1829 [==============================] - 307s 168ms/step - loss: 2.1903 - accuracy: 0.5884\n",
            "Epoch 88/100\n",
            "1829/1829 [==============================] - 308s 169ms/step - loss: 2.1707 - accuracy: 0.5918\n",
            "Epoch 89/100\n",
            "1829/1829 [==============================] - 309s 169ms/step - loss: 2.1540 - accuracy: 0.5946\n",
            "Epoch 90/100\n",
            "1829/1829 [==============================] - 308s 169ms/step - loss: 2.1366 - accuracy: 0.5985\n",
            "Epoch 91/100\n",
            "1829/1829 [==============================] - 308s 168ms/step - loss: 2.1163 - accuracy: 0.6011\n",
            "Epoch 92/100\n",
            "1829/1829 [==============================] - 308s 168ms/step - loss: 2.0986 - accuracy: 0.6047\n",
            "Epoch 93/100\n",
            "1829/1829 [==============================] - 310s 169ms/step - loss: 2.0828 - accuracy: 0.6076\n",
            "Epoch 94/100\n",
            "1829/1829 [==============================] - 309s 169ms/step - loss: 2.0662 - accuracy: 0.6105\n",
            "Epoch 95/100\n",
            "1829/1829 [==============================] - 310s 169ms/step - loss: 2.0483 - accuracy: 0.6131\n",
            "Epoch 96/100\n",
            "1829/1829 [==============================] - 310s 169ms/step - loss: 2.0325 - accuracy: 0.6164\n",
            "Epoch 97/100\n",
            "1829/1829 [==============================] - 309s 169ms/step - loss: 2.0183 - accuracy: 0.6191\n",
            "Epoch 98/100\n",
            "1829/1829 [==============================] - 310s 170ms/step - loss: 2.0036 - accuracy: 0.6217\n",
            "Epoch 99/100\n",
            "1829/1829 [==============================] - 310s 169ms/step - loss: 1.9852 - accuracy: 0.6256\n",
            "Epoch 100/100\n",
            "1829/1829 [==============================] - 311s 170ms/step - loss: 1.9712 - accuracy: 0.6278\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a38a0cef70>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' ,metrics = ['accuracy'] )\n",
        "model.fit(gen.generate()  , steps_per_epoch = int(234140/128) , epochs = 100 ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kwW2jOUUJ-G7"
      },
      "outputs": [],
      "source": [
        "model.save(\"language_model_twitter.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# concat indx_ , prediction\n",
        "indx_ = np.array([[129 , 95]])\n",
        "prediction = np.argmax(model.predict(indx_))\n",
        "indx_ = np.concatenate((indx_ , [[prediction]]), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[129,  95, 892]], dtype=int64)"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word = [inverse_vocab[i[0]] for i in indx_]\n",
        "indx_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "892"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = np.argmax(model.predict(indx_))\n",
        "indx_ = np.concatenate((indx_ , [[prediction]]), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['اللهم', 'الله', 'صباح', 'شيء']"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[inverse_vocab[i] for i in indx_.tolist()[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "612"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "RNN Language Model.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "c2b1884aac98f9b7855e8895f74d488ad518f49fce9c97dd1a17c527c00def65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
